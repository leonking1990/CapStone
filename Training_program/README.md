# Machine Learning Model Training (PlantCareAI)

This directory contains the Python scripts and associated resources for training the Convolutional Neural Network (CNN) models used by the PlantCareAI backend server for:

1.  **Plant Species Classification**
2.  **Plant Health Classification (Disease Detection)**

## Overview

The core training process is managed by `main.py`. It utilizes the `CNNModel` class defined in `CNNModel.py` for the model architecture and training loop. Data pipelines are handled by `tf_data_pipeline_new.py`, and various helper scripts for data preparation and analysis are located in `Data_management_scripts/`.

## Key Scripts

* **`main.py`**: The main entry point for training. It handles argument parsing, data loading (using the pipeline script), model creation (using `CNNModel`), training execution, evaluation, and saving the trained model and associated logs.
* **`CNNModel.py`**: Defines the `PlantCNNModel` class, which encapsulates the Keras model architecture (using EfficientNetV2B0 as a base by default), compilation, training methods (`train_model`), prediction methods (`predict_image`), and model saving/loading.
* **`tf_data_pipeline_new.py`**: Contains functions to create efficient `tf.data.Dataset` pipelines for loading and augmenting image data during training and validation.
* **`ImageProcesser.py`**: (If used in your workflow) A script potentially for initial image processing steps before creating the dataset.
* **`Data_management_scripts/`**: Contains various utility scripts:
    * `Train_vs_Val.py`: Splits the dataset into training and validation sets.
    * `Distribution_Analyzer.py`: Analyzes and visualizes class distribution in the dataset.
    * `Image_Organizer_less50.py`, `organizer.py`, `ImageOrganizer.py`: Scripts likely used for organizing image files into a structured directory format.
    * `Image_counter.py`, `random_image_sampler.py`: Other data handling utilities.

## Required Inputs

1.  **Image Dataset:** The training process requires a dataset of plant images organized into subdirectories, where each subdirectory name corresponds to a class label (e.g., `dataset/species_A/img1.jpg`, `dataset/species_B/img2.jpg`). The exact root path for the dataset needs to be provided as an argument to `main.py` (`--data_dir`). **You must provide and structure this dataset yourself.**
2.  **Class Indices (Optional Input):** While `main.py` can generate class indices, you might use pre-defined JSON files (like `class_indices.json` or `PlantClassifier_class_indices.json` found in this directory) to ensure consistency, potentially passed via arguments.

## Outputs

* **Trained Models (`.keras`):** Saved models are typically placed in a `models/` subdirectory within `Training_program/` (or as specified by arguments in `main.py`). **These `.keras` files need to be manually copied to the `CapStone/Server/models/` directory**, renamed appropriately (`plant_classifier.keras`, `health_classifier.keras`), to be used by the backend server.
* **Training Logs (`logs/`):** This directory stores outputs from training runs, organized by timestamp:
    * TensorBoard logs for visualizing training progress.
    * `training_log.csv`: CSV file logging metrics per epoch.
    * `class_weights.csv`: Records class weights used during training.
    * Confusion Matrix plots (if generated by `main.py`).
* **Class Mapping Files:** The training process should output the final class-to-index mapping used by the model. Ensure these are saved and **formatted correctly to be used as `species_map.json` and `disease_map.json` in the `CapStone/Server/mappings/` directory.** The server expects a specific JSON structure (see `Server/README.md`).

## Setup

1.  **Navigate to Training Directory:**
    ```bash
    cd path/to/CapStone/Training_program
    ```
2.  **Create Virtual Environment** (Recommended, can be separate from the server's venv):
    ```bash
    # Create
    python -m venv venv_train # Or python3 -m venv venv_train
    # Activate
    source venv_train/bin/activate # Linux/macOS
    # venv_train\Scripts\activate.bat # Windows Cmd
    # .\venv_train\Scripts\Activate.ps1 # Windows PowerShell
    ```
3.  **Install Requirements:**
    * Ensure you have a `requirements.txt` file in this directory with the necessary packages (TensorFlow, Pandas, scikit-learn, Pillow, Matplotlib, Seaborn, etc.). See the main project README or the analysis provided previously.
    * Install dependencies:
        ```bash
        pip install -r requirements.txt
        ```
4.  **Prepare Dataset:**
    * **Crucially, place your raw image data** in a location accessible by the scripts.
    * Use scripts from `Data_management_scripts/` (like `Train_vs_Val.py`, organizers) to process and structure your data into the required format (likely separate `train` and `validation` directories, each containing class subdirectories, e.g., `your_dataset_root/train/species_A/...`, `your_dataset_root/validation/species_A/...`).
    * Note the final path to the prepared dataset root directory (e.g., `your_dataset_root`).

## Running Training

The primary script for training is `main.py`.

1.  **Activate Virtual Environment:** If not already active.
2.  **Run `main.py`:** Use command-line arguments to configure the training run. You can see all options with `python main.py --help`. Key arguments likely include:
    * `--data_dir`: **Required.** Path to the prepared dataset directory (e.g., `/path/to/your_dataset_root`).
    * `--model_type`: **Required.** Specify `'species'` or `'health'` to train the respective classifier.
    * `--model_name`: Base name for saving the model and logs (e.g., 'PlantSpeciesEfficientNet_V1').
    * `--epochs`: Number of training epochs (e.g., `--epochs 50`).
    * `--batch_size`: Batch size for training (e.g., `--batch_size 32`).
    * `--learning_rate`: Initial learning rate (e.g., `--learning_rate 0.001`).
    * `--img_height`, `--img_width`: Image dimensions (default likely 224).
    * `--use_class_weights`: Flag to enable calculation and use of class weights for imbalanced datasets.
    * `--augmentation_level`: Specify level of data augmentation (`'none'`, `'basic'`, `'moderate'`).

    **Example Command (Species Model):**
    ```bash
    python main.py --data_dir /path/to/prepared_species_dataset --model_type species --model_name SpeciesClassifier_Run1 --epochs 50 --batch_size 32 --learning_rate 0.001 --use_class_weights --augmentation_level moderate
    ```
    **Example Command (Health Model):**
    ```bash
    python main.py --data_dir /path/to/prepared_health_dataset --model_type health --model_name HealthClassifier_Run1 --epochs 30 --batch_size 32 --learning_rate 0.001 --use_class_weights --augmentation_level basic
    ```

3.  **Monitor Training:** Observe the terminal output for progress per epoch. Check the timestamped subdirectory created within `logs/` for detailed logs and outputs. Use TensorBoard to visualize training metrics:
    ```bash
    # Run from the Training_program/ directory
    tensorboard --logdir logs
    ```
    Then open the provided URL (usually `http://localhost:6006/`) in your browser.

## Evaluation

The `main.py` script typically performs evaluation on the validation set after training completes (or potentially after each epoch). It logs metrics like accuracy, loss, precision, recall, F1-score to the console and the `training_log.csv` file. It may also save visualizations like a confusion matrix plot to the specific run's log directory.

## Dependencies

Ensure you have created and installed the packages from `requirements.txt` specific to this training program. Key dependencies include `tensorflow`, `pandas`, `scikit-learn`, `Pillow`, `matplotlib`, and `seaborn`.